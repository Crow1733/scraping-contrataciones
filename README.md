# ğŸ›ï¸ API de Licitaciones - contrataciondelestado.es

API REST construida con FastAPI para obtener licitaciones del portal oficial de contrataciÃ³n del estado espaÃ±ol mediante web scraping con Selenium.

## ğŸ“‹ CaracterÃ­sticas

- âœ… Scraping automÃ¡tico de licitaciones en tiempo real
- âœ… Filtrado por cÃ³digos CPV (ClasificaciÃ³n de Productos y Servicios)
- âœ… BÃºsqueda por rangos de fechas personalizables
- âœ… Filtro por estado (Publicada, EvaluaciÃ³n, Adjudicada, etc.)
- âœ… PaginaciÃ³n automÃ¡tica de resultados
- âœ… ExportaciÃ³n a CSV y JSON
- âœ… DocumentaciÃ³n interactiva (Swagger UI)
- âœ… Modo headless (sin interfaz grÃ¡fica)

## ğŸš€ InstalaciÃ³n

### Requisitos previos

**Windows:**
- Python 3.8+
- Google Chrome instalado
- pip

**Ubuntu/Linux:**
```bash
# Instalar Chrome
wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb
sudo apt install ./google-chrome-stable_current_amd64.deb

# Dependencias de Chrome headless
sudo apt install -y libnss3 libatk-bridge2.0-0t64 libdrm2 libxkbcommon0 libgbm1 libasound2t64
```

### Pasos de instalaciÃ³n

1. **Clonar el repositorio:**
```bash
git clone https://github.com/tu-usuario/scraping-contrataciones.git
cd scraping-contrataciones
```

2. **Crear entorno virtual:**
```bash
# Windows
python -m venv venv
.\venv\Scripts\activate

# Linux/Ubuntu
python3 -m venv venv
source venv/bin/activate
```

3. **Instalar dependencias:**
```bash
pip install -r requirements.txt
```

## ğŸ¯ Uso

### Iniciar el servidor

```bash
python main.py
```

El servidor estarÃ¡ disponible en: `http://localhost:8000`

**DocumentaciÃ³n interactiva:** `http://localhost:8000/docs`

### Endpoints disponibles

#### **GET /** - InformaciÃ³n de la API
```bash
curl http://localhost:8000/
```

#### **GET /health** - Estado del servicio
```bash
curl http://localhost:8000/health
```

#### **GET /licitaciones** - Obtener licitaciones

**Sin parÃ¡metros** (licitaciones de hoy):
```bash
curl http://localhost:8000/licitaciones
```

**Con cÃ³digos CPV** (Software y Servicios TI):
```bash
curl "http://localhost:8000/licitaciones?cpv_codes=48000000,72000000"
```

**Con rango de fechas:**
```bash
curl "http://localhost:8000/licitaciones?fecha_desde=01-01-2026&fecha_hasta=31-01-2026"
```

**Ejemplo completo:**
```bash
curl "http://localhost:8000/licitaciones?cpv_codes=48000000&fecha_desde=10-01-2026&fecha_hasta=15-01-2026"
```

### Respuesta ejemplo

```json
{
  "success": true,
  "timestamp": "2026-01-16T10:30:00",
  "total_licitaciones": 25,
  "codigos_cpv": ["48000000", "72000000"],
  "fecha_desde": "15-01-2026",
  "fecha_hasta": "15-01-2026",
  "licitaciones": [
    {
      "expediente": "2026/001234",
      "descripcion": "Servicio de desarrollo de aplicaciÃ³n web",
      "tipo": "Servicios",
      "subtipo": "Servicios informÃ¡ticos",
      "estado": "Publicada",
      "importe": "50,000.00",
      "fecha": "15/01/2026",
      "organismo": "Ministerio de...",
      "enlace": "https://contrataciondelestado.es/..."
    }
  ]
}
```

## ğŸ“‚ Estructura del proyecto

```
scraping-contrataciones/
â”œâ”€â”€ main.py                 # FastAPI app principal
â”œâ”€â”€ scraper_selenium.py     # LÃ³gica de scraping con Selenium
â”œâ”€â”€ config.py               # ConfiguraciÃ³n y constantes
â”œâ”€â”€ logger.py               # Sistema de logging
â”œâ”€â”€ requirements.txt        # Dependencias Python
â”œâ”€â”€ .gitignore             # Archivos ignorados por Git
â”œâ”€â”€ README.md              # Este archivo
â”œâ”€â”€ datos_licitaciones/    # Datos extraÃ­dos (CSV/JSON)
â””â”€â”€ logs/                  # Archivos de log
```

## ğŸ”§ ConfiguraciÃ³n

### Variables de entorno (opcional)

Puedes crear un archivo `.env` para configuraciones personalizadas:

```env
# API Configuration
API_HOST=127.0.0.1
API_PORT=8000

# Scraping Configuration
HEADLESS=true
TIMEOUT=120
```

## ğŸ§ Despliegue en Ubuntu Server

### 1. Configurar el servidor

```bash
# Actualizar sistema
sudo apt update && sudo apt upgrade -y

# Instalar Chrome y dependencias (ver secciÃ³n de requisitos)
```

### 2. Subir archivos

```bash
# Desde Windows (PowerShell)
scp -r *.py *.txt *.md root@TU_IP:/root/scraping-api/
```

### 3. Configurar y ejecutar

```bash
cd /root/scraping-api
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
python3 main.py
```

### 4. Ejecutar como servicio (systemd)

Crear `/etc/systemd/system/scraping-api.service`:

```ini
[Unit]
Description=API de Licitaciones
After=network.target

[Service]
Type=simple
User=root
WorkingDirectory=/root/scraping-api
Environment="PATH=/root/scraping-api/venv/bin"
ExecStart=/root/scraping-api/venv/bin/python3 main.py
Restart=always

[Install]
WantedBy=multi-user.target
```

Activar servicio:
```bash
sudo systemctl daemon-reload
sudo systemctl enable scraping-api
sudo systemctl start scraping-api
sudo systemctl status scraping-api
```

## ğŸ“Š Logs y debugging

Los logs se guardan automÃ¡ticamente en:
- `logs/scraping_YYYYMMDD.log` - Log general
- Capturas de pantalla en `datos_licitaciones/timestamp_id/`

Ver logs en tiempo real:
```bash
tail -f logs/scraping_*.log
```

## ğŸ›¡ï¸ Seguridad

**IMPORTANTE**: Para producciÃ³n, configura:

1. **Cambiar host a localhost:**
```python
# main.py
uvicorn.run("main:app", host="127.0.0.1", port=8000, reload=False)
```

2. **Implementar autenticaciÃ³n** (API Key, OAuth2)
3. **Usar reverse proxy** (Nginx con HTTPS)
4. **Configurar firewall** (UFW en Ubuntu)

## ğŸ¤ Contribuir

1. Fork el proyecto
2. Crea una rama (`git checkout -b feature/nueva-funcionalidad`)
3. Commit cambios (`git commit -am 'Agregar nueva funcionalidad'`)
4. Push a la rama (`git push origin feature/nueva-funcionalidad`)
5. Abre un Pull Request

## ğŸ“ Licencia

Este proyecto es de cÃ³digo abierto para fines educativos.

## âš ï¸ Disclaimer

Este proyecto realiza web scraping del portal pÃºblico contrataciondelestado.es. AsegÃºrate de cumplir con los tÃ©rminos de uso del sitio web y no realizar peticiones excesivas que puedan afectar su disponibilidad.

## ğŸ“ Soporte

Para reportar problemas o sugerencias, abre un issue en GitHub.

---

**Desarrollado con â¤ï¸ usando FastAPI y Selenium**
